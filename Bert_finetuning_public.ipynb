{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Objective:\n",
    "    -1. Finetune BERT model using glance training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U pip setuptools \n",
    "!pip3 install -U sentence-transformers\n",
    "!pip3 install -U torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import *\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from typing import Union, List\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InputExample:\n",
    "    \"\"\"\n",
    "    Structure for one input example with texts, the label and a unique id\n",
    "    \"\"\"\n",
    "    def __init__(self, guid: str, texts: List[str], label: Union[int, float]):\n",
    "        \"\"\"\n",
    "        Creates one InputExample with the given texts, guid and label\n",
    "        str.strip() is called on both texts.\n",
    "        :param guid\n",
    "            id for the example\n",
    "        :param texts\n",
    "            the texts for the example\n",
    "        :param label\n",
    "            the label for the example\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.texts = [text.strip() for text in texts]\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class to read training data for finetuning\n",
    "\n",
    "class DataReader(object):\n",
    "    \"\"\"\n",
    "    Reads in the dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_folder):\n",
    "        self.dataset_folder = dataset_folder\n",
    "\n",
    "    def get_examples(self, filename, max_examples=0):\n",
    "        \n",
    "        \"\"\"\n",
    "        Loads three files, the first the the sentence 1 the second with\n",
    "        sentence 2 and the last with the label connecting the sentence 1 and sentence 2\n",
    "       \n",
    "        \"\"\"\n",
    "        s1 = gzip.open(os.path.join(self.dataset_folder, 's1.' + filename),\n",
    "                       mode=\"rt\", encoding=\"utf-8\").readlines()\n",
    "        s2 = gzip.open(os.path.join(self.dataset_folder, 's2.' + filename),\n",
    "                       mode=\"rt\", encoding=\"utf-8\").readlines()\n",
    "        labels = gzip.open(os.path.join(self.dataset_folder, 'labels.' + filename),\n",
    "                           mode=\"rt\", encoding=\"utf-8\").readlines()\n",
    "        examples = []\n",
    "        id = 0\n",
    "        for sentence_a, sentence_b, label in zip(s1, s2, labels):\n",
    "            guid = \"%s-%d\" % (filename, id)\n",
    "            id += 1\n",
    "            examples.append(InputExample(guid=guid, texts=[sentence_a, sentence_b], label=self.map_label(label)))\n",
    "\n",
    "            if 0 < max_examples <= len(examples):\n",
    "                break\n",
    "\n",
    "        return examples\n",
    "\n",
    "    @staticmethod\n",
    "    def get_labels():\n",
    "        return {\"contradiction\": 0, \"entailment\": 1}\n",
    "\n",
    "    def get_num_labels(self):\n",
    "        return len(self.get_labels())\n",
    "\n",
    "    def map_label(self, label):\n",
    "        return self.get_labels()[label.strip().lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class to read Semantic Textual Similarity data for evaluation\n",
    "\n",
    "class STSDataReader:\n",
    "    \"\"\"\n",
    "    Reads in the STS dataset. Each line contains two sentences (s1_col_idx, s2_col_idx) and one label (score_col_idx)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_folder, s1_col_idx=5, s2_col_idx=6, score_col_idx=4, delimiter=\"\\t\",\n",
    "                 quoting=csv.QUOTE_NONE, normalize_scores=True, min_score=0, max_score=5):\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.score_col_idx = score_col_idx\n",
    "        self.s1_col_idx = s1_col_idx\n",
    "        self.s2_col_idx = s2_col_idx\n",
    "        self.delimiter = delimiter\n",
    "        self.quoting = quoting\n",
    "        self.normalize_scores = normalize_scores\n",
    "        self.min_score = min_score\n",
    "        self.max_score = max_score\n",
    "\n",
    "    def get_examples(self, filename, max_examples=0):\n",
    "        \"\"\"\n",
    "        filename specified which data split to use (train.csv, dev.csv, test.csv).\n",
    "        \"\"\"\n",
    "        data = csv.reader(open(os.path.join(self.dataset_folder, filename), encoding=\"utf-8\"),\n",
    "                          delimiter=self.delimiter, quoting=self.quoting)\n",
    "        examples = []\n",
    "        for id, row in enumerate(data):\n",
    "            score = float(row[self.score_col_idx])\n",
    "            if self.normalize_scores:  # Normalize to a 0...1 value\n",
    "                score = (score - self.min_score) / (self.max_score - self.min_score)\n",
    "\n",
    "            s1 = row[self.s1_col_idx]\n",
    "            s2 = row[self.s2_col_idx]\n",
    "            examples.append(InputExample(guid=filename+str(id), texts=[s1, s2], label=score))\n",
    "\n",
    "            if max_examples > 0 and len(examples) >= max_examples:\n",
    "                break\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_base_bert_model(base_model='bert-base-uncased',batch_size=20,num_epochs=4,evaluation_steps=1000,train_data_path_folder='Bert_Finetune_data/Training/' ,\n",
    "                             train_data_path_file='train_1.gz',eval_data_path_folder='Bert_Finetune_data/Eval_sts/',eval_data_path_file='sts-dev.csv',\n",
    "                            model_save_path='Bert_Finetune_data/Bert_fine_tune_results_nli_mean_base/Results/'):\n",
    "    \n",
    "    if base_model =='bert-base-uncased':\n",
    "        \n",
    "        print(\"Finetuning base uncased model\")\n",
    "        \n",
    "        # Read the dataset\n",
    "        model_name = base_model\n",
    "        batch_size = batch_size\n",
    "\n",
    "        # Use BERT for mapping tokens to embeddings\n",
    "        word_embedding_model = models.BERT(model_name)\n",
    "\n",
    "        # Apply mean pooling to get one fixed sized sentence vector\n",
    "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                                       pooling_mode_mean_tokens=True,\n",
    "                                       pooling_mode_cls_token=False,\n",
    "                                       pooling_mode_max_tokens=False)\n",
    "\n",
    "        model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "        \n",
    "        print(\"Reading Training data\")\n",
    "        \n",
    "        nli_reader = DataReader(train_data_path_folder)\n",
    "\n",
    "        train_num_labels = 2\n",
    "        \n",
    "        train_data = SentencesDataset(nli_reader.get_examples(train_data_path_file), model=model)\n",
    "        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "        train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n",
    "\n",
    "        print(\"Reading Eval data\")\n",
    "\n",
    "        sts_reader = STSDataReader(eval_data_path_folder)\n",
    "\n",
    "        dev_data = SentencesDataset(examples=sts_reader.get_examples(eval_data_path_file), model=model)\n",
    "        dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=batch_size)\n",
    "        evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n",
    "        \n",
    "        model_save_path = model_save_path\n",
    "        \n",
    "        print(\"Starting Training..\")\n",
    "\n",
    "        # Configure the training\n",
    "        num_epochs = num_epochs\n",
    "\n",
    "        warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "                  evaluator=evaluator,\n",
    "                  epochs=num_epochs,\n",
    "                  evaluation_steps=evaluation_steps,\n",
    "                  warmup_steps=warmup_steps,\n",
    "                  output_path=model_save_path\n",
    "                  )\n",
    "        \n",
    "        model_fine_tuned_base = SentenceTransformer(model_save_path)\n",
    "        \n",
    "        print(\"Finished finetuning of base model\")\n",
    "\n",
    "        return model\n",
    "\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Read the dataset\n",
    "        model_name = base_model\n",
    "        \n",
    "        train_batch_size = batch_size\n",
    "        \n",
    "        # Load a pre-trained sentence transformer model\n",
    "        model = SentenceTransformer(model_name)\n",
    "        \n",
    "        print(\"Finetuning base model\")\n",
    "\n",
    "        \n",
    "        print(\"Reading Training data\")\n",
    "\n",
    "        nli_reader = DataReader(train_data_path_folder)\n",
    "        \n",
    "        train_num_labels = 2\n",
    "        \n",
    "        train_data = SentencesDataset(nli_reader.get_examples(train_data_path_file), model=model)\n",
    "        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "        train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n",
    "\n",
    "        print(\"Reading Eval data\")\n",
    "\n",
    "        sts_reader = STSDataReader(eval_data_path_folder)\n",
    "\n",
    "        dev_data = SentencesDataset(examples=sts_reader.get_examples(eval_data_path_file), model=model)\n",
    "        dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=batch_size)\n",
    "        evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n",
    "        \n",
    "        model_save_path = model_save_path\n",
    "        \n",
    "        print(\"Starting Training..\")\n",
    "\n",
    "        \n",
    "        # Configure the training\n",
    "        num_epochs = num_epochs\n",
    "\n",
    "        warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "                  evaluator=evaluator,\n",
    "                  epochs=num_epochs,\n",
    "                  evaluation_steps=evaluation_steps,\n",
    "                  warmup_steps=warmup_steps,\n",
    "                  output_path=model_save_path\n",
    "                  )\n",
    "                \n",
    "        print(\"Finished finetuning of base model\")\n",
    "        \n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
